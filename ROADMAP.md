# Axiom Agent: Development Roadmap

This document outlines the current status, planned features, and architectural improvements for the Axiom Agent project.

## âœ… Genesis Phase: Autonomous Local Agent (Complete & Stable)

This foundational phase is **complete**. The project has successfully evolved from a prototype into a stable, high-performance cognitive agent capable of autonomous learning, logical reasoning, and robust deployment, all on local CPU hardware. This phase serves as the stable bedrock for all future development.

### âœ… **Core Architecture & Capabilities**
- **Hybrid Cognitive Architecture:** Combines a symbolic **Knowledge Graph** (NetworkX) with a neural **Universal Interpreter** (LLM) for verifiable memory and flexible understanding.
- **Multi-Hop Logical Reasoning:** Traverses the knowledge graph to infer conclusions from connected facts.
- **Dual-Cycle Autonomous Learning:** Employs a "Cognitive Scheduler" for continuous self-improvement:
    - **Study Cycle:** Introspectively reviews and enriches existing knowledge.
    - **Discovery Cycle:** Actively seeks out and learns entirely new topics.
- **Dual-Mode Learning System:** Possesses both an autonomous **Curiosity Engine** for resolving internal conflicts and a **User-Driven Correction** mechanism for explicit instruction.
- **Resilient Knowledge Harvester:** Uses multiple sources (Wikipedia, DDG) and intelligent filters to acquire new, high-quality knowledge.
- **Vast Knowledge Base Seeding:** Initializes with a large, pre-seeded set of foundational knowledge.

### âœ… **Scalability & Performance**
- **Knowledge Graph Engine Upgrade:** The core graph is powered by the battle-tested, C-optimized **`NetworkX`** engine, ensuring scalability.
- **Multi-Layer Caching System:** Drastically reduces query latency and LLM calls via in-memory **Reasoning Caches** (`lru_cache`) and on-disk **Interpreter/Synthesizer Caches**.
- **Bombproof Stability:** All critical, show-stopping bugs (segmentation faults, amnesia on restart, autonomous cycle failures) have been identified and **permanently fixed**.
- **Verified Performance:** Profiling confirms Python-level bottlenecks are eliminated, with cached queries resolving in **under 0.01 seconds**.

### âœ… **Professional Development & Deployment Workflow**
- **Structured Project Layout:** The codebase has been professionally reorganized into a clean package structure (`axiom/`, `setup/`, `tests/`, etc.) for maintainability and scalability.
- **Centralized Configuration (`pyproject.toml`):** The project now uses the modern standard for managing dependencies and tool settings, replacing multiple legacy `requirements.txt` files.
- **Automated Quality Assurance (`check.sh`):** A comprehensive check script automates a full suite of software quality tests with a single command:
    - **Code Formatting (`Ruff Format`):** Enforces a consistent, professional code style across the entire project.
    - **Linting (`Ruff Check`):** Statically analyzes code to catch potential bugs, style errors, and anti-patterns before they become runtime issues.
    - **Static Type Checking (`MyPy`):** Verifies type hints to prevent a whole class of common bugs and improve code reliability.
    - **Unit Testing (`Pytest`):** Runs an automated test suite to ensure core components function as expected.
- **Versioned Model Rendering (`.axm`):** A custom "Axiom Mind" format packages the agent's brain and cache into a single, portable snapshot.
- **Complete Toolchain:** A full suite of scripts enables a professional "Offline Training, Online Inference" workflow:
    - **`autonomous_trainer.py`:** A headless script for 24/7 autonomous learning.
    - **`cnt.py`:** A command-line tool for manual, interactive training.
    - **`render_model.py`:** Packages the trained brain into a new `.axm` model.
    - **`app_model.py`:** A read-only Flask server that deploys the latest model for safe user interaction.

### âœ… **User Interface & Experience**
- **Multi-Conversation Management:** A robust UI with persistent chat sessions saved to local storage.
- **Full PWA (Progressive Web App) Support:** The agent is installable on desktop and mobile for a native-like experience.
- **Thread-Safe Operation:** A global lock ensures memory integrity during simultaneous user chats and autonomous learning cycles.

---

## ðŸ›‘ On Hold / Deprecated Features

*Features that are architecturally sound but have been temporarily disabled to prioritize core stability and performance. They can be revisited in future development phases.*

### 1. Dynamic Fact Salience (`access_count`)
- **Status:** **`On Hold / Partially Deprecated`**. Incompatible with the current high-performance `lru_cache` system.
- **Path Forward:** Can be re-introduced with a more sophisticated, cache-aware architecture.

### 2. Conversational Context (Short-Term Memory)
- **Status:** **`On Hold / Disabled`**. The initial LLM-based approach proved unreliable.
- **Path Forward:** Requires a complete architectural rethink, potentially leveraging a more powerful base model or a different logical framework.

---

## Future Development Roadmap (Next 12 Months)

With the local agent now stable and robust, the strategic focus shifts to **overcoming the physical limitations of a single machine**. The next phases will leverage free-tier cloud resources to give the agent a massively scaled memory and enhanced cognitive abilities, all without incurring costs.

### Free Cloud GPU/Resource Strategy

The entire future roadmap is designed to be executed using a suite of powerful, free-tier cloud services. This allows for massive scaling without financial investment.
- **Primary Tools:** Google Colab, Kaggle Kernels, AWS SageMaker Studio Lab.
- **Persistent Storage:** Google Drive.
- **Strategy:** Distribute computationally intensive tasks (large-scale graph queries, model fine-tuning) to these services while keeping the core agent logic portable.

---

### **Phase 2: The Distributed Mind (Cloud Knowledge Graph Integration)**
- **Goal:** Overcome local RAM/storage limits by migrating the knowledge graph to a free-tier cloud database, enabling the agent to scale its memory to hundreds of thousands of concepts.
- **Milestone:** Agent's brain lives in a persistent, scalable cloud database (e.g., Neo4j AuraDB Free, Redis Cloud).
- **Key Steps:**
    1.  **Migrate Graph Storage:** Replace `NetworkX` file I/O in `graph_core.py` with a connector to a cloud graph database.
    2.  **Refactor Learning Cycles:** Update `knowledge_harvester.py` to perform queries and writes directly against the cloud database.
    3.  **Implement Efficient Syncing:** Develop a mechanism in `cloud_utils.py` to sync or "cache" relevant sub-graphs from the cloud for fast local reasoning.
    4.  **Introduce Graph Pruning:** Implement a "salience decay" mechanism to periodically prune old, unused edges from the cloud graph to keep it lean.
- **Success Metrics:** Agent can successfully read/write to the cloud graph. The local agent can operate with a small memory footprint by only caching active sub-graphs. Graph size can grow beyond local machine limits.

### **Phase 3: The Enlightened Interpreter (LLM Independence & Accuracy)**
- **Goal:** Reduce reliance on a single, general-purpose LLM for interpretation, leading to higher accuracy, fewer errors, and the ability to process more complex information.
- **Milestone:** The `UniversalInterpreter` evolves into a hybrid, rule-based/ensemble system.
- **Key Steps:**
    1.  **Implement a Hybrid Parser:** Integrate a fast, local NLP library (like `spaCy`) into `universal_interpreter.py` to handle simple, unambiguous sentences without needing an LLM call.
    2.  **Develop an Ensemble Model:** For complex sentences, query multiple small, efficient models (e.g., Phi-2, Gemma 2B running in a Colab GPU instance) and use a "voting" system to determine the most likely correct interpretation.
    3.  **Self-Correction via Web Verification:** When the agent detects a knowledge conflict, it will use its cloud-bursting capability to perform a web search to verify the conflicting facts and autonomously correct its own brain.
    4.  **Fine-Tuning:** Use the history of successful interpretations to generate a dataset for fine-tuning a small, specialized interpretation model in SageMaker Studio Lab.
- **Success Metrics:** >60% of simple statements are interpreted locally without an LLM call. Interpretation accuracy for complex statements increases to >90%.

### **Phase 4: The Autonomous Scholar (Advanced Curriculum Learning)**
- **Goal:** Evolve the agent's learning from random discovery to a strategic, goal-oriented "curriculum."
- **Milestone:** The agent can autonomously set and pursue learning goals.
- **Key Steps:**
    1.  **Develop a Goal System:** Implement a mechanism for the agent to set high-level learning goals (e.g., "Understand quantum physics").
    2.  **Curriculum Generation:** When a goal is set, the agent will use its tools to generate a curriculum of prerequisite topics (e.g., "First, I must learn what an atom is. Then, what a subatomic particle is.").
    3.  **Reinforcement Heuristics:** Implement a simple reinforcement system in `rl_heuristics.py` that "rewards" the agent for learning facts related to its current curriculum goal, making it more likely to study related topics.
- **Success Metrics:** The agent demonstrates the ability to learn a complex topic by systematically exploring its foundational concepts first. The knowledge graph shows dense, interconnected clusters of knowledge around specific domains.
