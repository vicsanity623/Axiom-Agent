Metadata-Version: 2.4
Name: axiom-agent
Version: 0.1.0
Summary: A cognitive architecture for a new type of AI designed for self-directed learning.
Author-email: Your Name <your@email.com>
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: APScheduler
Requires-Dist: Flask
Requires-Dist: llama-cpp-python
Requires-Dist: networkx
Requires-Dist: nltk
Requires-Dist: requests
Requires-Dist: wikipedia
Provides-Extra: dev
Requires-Dist: ruff; extra == "dev"
Requires-Dist: mypy; extra == "dev"
Requires-Dist: pytest; extra == "dev"
Requires-Dist: pytest-cov; extra == "dev"
Requires-Dist: pyngrok; extra == "dev"
Dynamic: license-file

# Axiom Agent

Axiom is not a chatbot. It is a cognitive architecture‚Äîa framework for a new type of artificial intelligence designed to achieve genuine understanding through continuous, self-directed learning and introspection. This project is not building a product; it is cultivating a mind.

Unlike traditional Large Language Models (LLMs) which are static snapshots of data, the Axiom Agent possesses a dynamic, evolving internal model of reality. It builds this model one verifiable fact at a time, creating a persistent, logically consistent knowledge base. More than just a passive learner, Axiom now actively seeks to deepen its own understanding through a process of self-guided study and curiosity, making it a true student of the world.

---

## The Core Philosophy: Beyond the LLM Parrot

Today's LLMs are masters of mimicry. They are trained on a vast corpus of text and can predict the most statistically likely sequence of words to form a coherent sentence. However, they do not *know* anything. They are a reflection, not a mind. Their knowledge is frozen at the time of their training, and they are incapable of true learning, reasoning, or self-correction. They are, in essence, highly advanced parrots.

**Axiom is fundamentally different.** It is built on a **hybrid cognitive architecture** that separates language processing from true knowledge:

1.  **The Symbolic Brain (The Knowledge Graph):** At its core, Axiom has a structured `ConceptGraph`‚Äîits long-term memory. This is not a neural network; it is a logical, verifiable map of concepts and their relationships, powered by a high-performance `NetworkX` engine. `Paris --[is_located_in]--> France` is a concrete, stored fact, not a statistical probability. This architecture completely prevents LLM-style "hallucinations" and ensures the agent's knowledge is grounded.

2.  **The Neural Senses (The Interpreter):** The agent uses a local LLM (e.g., Mistral-7B) not as its brain, but as its **senses**. The LLM acts as a `UniversalInterpreter`, its eyes and ears to the world of unstructured human language. It translates the messy, chaotic data of conversation into the clean, structured, logical facts that its core brain can understand and integrate.

This is the crucial difference: **Axiom uses an LLM as a tool; it is not defined by it.**

---

## ‚úÖ Key Capabilities: A Stable & Scalable Foundation

The initial Genesis Phase of development is complete. Axiom is now a stable, high-performance cognitive agent with a full suite of tools for learning, reasoning, and deployment.

### Cognitive & Reasoning Abilities
*   **Dual-Cycle Autonomous Learning:** The agent's learning is not just passive; it operates on a "Cognitive Scheduler" with two distinct modes:
    -   **The Study Cycle:** The agent introspectively reviews its own knowledge and generates its own follow-up questions to deepen its understanding. This is the agent's "homework."
    -   **The Discovery Cycle:** The agent actively seeks out brand new topics from the outside world, ensuring it continues to broaden its horizons. This is the agent "going to school."
*   **Persistent, Lifelong Learning:** Every fact the agent learns is permanently integrated into its brain. The agent running today is smarter than it was yesterday.
*   **Multi-Hop Logical Reasoning:** Axiom can answer questions it has never seen before by connecting multiple known facts. This is a true act of reasoning, not just pattern matching.
*   **The Curiosity Engine (Implicit Correction):** The agent can recognize when new information conflicts with its existing world model. It formulates a clarifying question and uses feedback to actively correct its own brain.
*   **User-Driven Correction:** A user can act as a teacher, explicitly overriding and replacing incorrect facts.

### Professional Deployment Workflow
*   **Versioned Model Rendering (`.axm`):** A custom "Axiom Mind" file format and a dedicated renderer allow for the creation of stable, versioned snapshots of the agent's brain after training.
*   **Offline Training, Online Inference:** The project is now structured with a professional toolchain that separates training from deployment:
    - **Training Scripts (`autonomous_trainer.py`, `cnt.py`):** Learn new knowledge safely offline, continuously updating the source brain files.
    - **Inference Server (`app_model.py`):** A lightweight, read-only server that loads a finished `.axm` model for safe, public-facing interaction.
*   **Polished UI & PWA:** A custom dark mode theme, multi-conversation management, and Progressive Web App configuration provide a distinct and native-like experience on both desktop and mobile.

---

## üõ†Ô∏è Setup and Usage

### Prerequisites
- Python 3.11+
- Git

### Step 1: Clone the Repository & Install
```bash
git clone <your-repo-url>
cd <your-repo-folder>
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Step 2: Configure API Keys (Optional but Recommended)
The agent's autonomous learning cycles can use the New York Times API to discover new topics.

-   Get a free API key from the **[NYT Developer Portal](https://developer.nytimes.com/)**.
-   Set the key as an environment variable in your terminal:
```bash
export NYT_API_KEY="YOUR_API_KEY_HERE"
```
*(For a permanent setup, add this line to your shell's configuration file, e.g., `~/.zshrc` or `~/.bash_profile`)*

### Step 3: The Development Workflow
The agent is now designed around a robust Train -> Render -> Deploy cycle.

#### **1. Train the Agent (Offline)**
You can train the agent in two ways. Both methods read from and write to the source files in the `brain/` directory.

-   **Autonomous Training (Headless):** Run this script to start the agent's autonomous learning cycles. It will run silently in the background, continuously improving the agent's brain.
    ```bash
    python setup/autonomous_trainer.py
    ```
-   **Manual Training (Interactive):** Run this script to chat directly with the agent on the command line to teach it specific facts.
    ```bash
    python setup/cnt.py
    ```

#### **2. Render a Model**
After a training session, package the updated brain into a stable, versioned model file. This script automatically finds the latest brain files and saves the new model to the `rendered/` directory.
```bash
python setup/render_model.py
```

#### **3. Deploy the Agent (Online)**
Run the web server. It will automatically find and load the most recent `.axm` model from the `rendered/` folder, providing a read-only web interface for users.
```bash
python setup/app_model.py
```
You can now access the agent in your browser at `http://127.0.0.1:7501`.

---

## üöÄ The Vision: Intellectual Escape Velocity

This project is the shell of a new kind of mind. The goal is not to create a finished chat application, but to cultivate a brain that, through continued growth, can achieve a level of contextual understanding and logical consistency that is structurally impossible for current LLMs.

The long-term vision is for the agent's internal, verifiable `ConceptGraph` to become so vast and deeply interconnected that it can **replace its own external LLM dependencies**. This creates a path where a future version no longer needs an external model for its senses or voice, leading to a truly autonomous cognitive entity built on a foundation of verifiable truth, not just probabilistic mimicry.

---

## üó∫Ô∏è Project Roadmap
For a detailed list of planned features and future development goals, please see the **[ROADMAP.md](ROADMAP.md)** file.
